# Pretrained Tokenizers

## Setup

```elixir
Mix.install([
  {:kino, "~> 0.5.2"},
  {:scidata, "~> 0.1.5"},
  {:tokenizers, path: "./"}
])
```

## Get a tokenizer

```elixir
{:ok, tokenizer} = Tokenizers.from_pretrained("bert-base-cased")
```

## Save and load

```elixir
input = Kino.Input.text("Path")
```

```elixir
path = Kino.Input.read(input)
Tokenizers.save(tokenizer, path)
```

```elixir
{:ok, tokenizer} = Tokenizers.from_file(path)
```

## Check the tokenizer

```elixir
{:ok, vocab} = Tokenizers.get_vocab(tokenizer)
```

```elixir
vocab["Jaguar"]
```

```elixir
Tokenizers.token_to_id(tokenizer, "Jaguar")
```

```elixir
Tokenizers.id_to_token(tokenizer, 21694)
```

```elixir
Tokenizers.get_vocab_size(tokenizer)
```

```elixir
map_size(vocab)
```

## Encode and decode

```elixir
{:ok, encoding} = Tokenizers.encode(tokenizer, "Hello there!")
```

```elixir
Tokenizers.get_tokens(encoding)
```

```elixir
{:ok, ids} = Tokenizers.get_ids(encoding)
```

```elixir
Tokenizers.decode(tokenizer, ids)
```

```elixir
{:ok, encodings} = Tokenizers.encode(tokenizer, ["Hello there!", "This is a test."])
```

```elixir
list_of_ids =
  Enum.map(encodings, fn encoding ->
    {:ok, ids} = Tokenizers.get_ids(encoding)
    ids
  end)
```

```elixir
Tokenizers.decode(tokenizer, list_of_ids)
```

## Get a tensor

In order to get a tensor, we need sequences that are all of the same length. We'll get some data from `Scidata` and use `Tokenizers.pad/3` and `Tokenizers.truncate/3` to yield a tensor.

```elixir
%{review: reviews} = Scidata.YelpPolarityReviews.download_test()
```

```elixir
tensor =
  reviews
  |> Enum.take(10)
  |> Enum.map(fn review ->
    {:ok, tokenized} = Tokenizers.encode(tokenizer, review)
    {:ok, padded} = Tokenizers.pad(tokenized, 200)
    {:ok, truncated} = Tokenizers.truncate(padded, 200, [])
    {:ok, indices} = Tokenizers.get_ids(truncated)
    indices
  end)
  |> Nx.tensor()
```

And we can reverse the operation to see our data. Note the `[PAD]` tokens.

```elixir
tensor
|> Nx.to_batched_list(1)
|> Enum.map(fn tensor_review ->
  {:ok, decoded} =
    tensor_review
    |> Nx.to_flat_list()
    |> then(&Tokenizers.decode(tokenizer, &1))

  decoded
end)
```
